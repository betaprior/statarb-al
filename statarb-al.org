#+TITLE: Statistical arbitrage with pairs trading
#+AUTHOR: Leo Alekseyev
#+property: session *R-babel*

* Introduction

Back when I was still in grad school on the East Coast, I couldn't help
noticing a steady stream of freshly-minted PhDs (or even ABDs) in physics,
math, CS, and engineering taking residence 1.5 hours up north on the train
line, and converting their knowledge of applied probability theory into cold,
hard cash.  Before getting acquainted with the field of financial engineering
first-hand, I had a rather distorted view of the /who/s and the /what/s of
capital markets (shaped, in part, by growing up in Soviet Russia and doing
undergrad in California).  To be sure, the finance industry can seem complex
and alienating. A significant part of the field, however, is, essentially,
comprised by our fellow geeks crunching numbers on beefy servers using the
open source tools we know and love.

One fun side project I've worked on a couple of years ago involved
reproducing a trading strategy described in a 
[[http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1153505][paper by Marco Avellaneda and Jeong-Hyun Lee]].  This paper is cool for several reasons:
- the math is interesting without being overly arcane
- it's fairly easy to implement in R
- there are many interesting things to explore both in terms of optimizing
  the trading strategy and in studying some fundamental aspects of financial
  time series
- if you had a time machine, you could make a killing with this strategy 5-10
  years ago!  (Yeah, Gray's Sports Almanac is for those /amateur/ time
  travelers.)

I recently stumbled upon the project source tree when cleaning up some git
repositories. After looking at the code, I had a couple of realizations:
- my coding style in both R and C++ has evolved quite a bit over the last
  couple of years
- this would be an awesome project to share with the world using org-mode's
  reproducible research/literate programming features  
  
As a result, I've started putting together a public version of this project.
My goal is to enable interested readers to follow the implementation
details step by step, and be able to experiment with the code freely. The
code itself, currently, is not far from its starting point as a one-off
scrappy personal project (this means I am not using =package.skeleton= or
ROxygen). The current focus is on getting everything to run smoothly under
org-babel. So far, the signal generation routines work well; the trading
simulation should be up shortly.  So if you want to get the flavor of the
sorts of things Wall Street quants do, grab a fresh version of org-mode,
clone this project from Github, grab the data files, load =statarb-al.org=
into Emacs, and continue following along in an org-mode buffer!

A quick note on where to get the tools used here: if you are on Windows or OS
X, and don't already use Emacs and/or Org, Vincent Goulet [[http://vgoulet.act.ulaval.ca/en/emacs/windows/][maintains a version]]
of Emacs with Org, AUCTeX, and ESS integrated. R is available from
[[http://r-project.org]]. Under Ubuntu, of course, you can get everything from
Synaptic (search for /r-base/ to get R; also, if the org-mode version in the
repository is < 7.8, you might want to get a more up-to-date version from
[[http://orgmode.org]]).

* Pairs trading and mean reversion: a preview
** What is pairs trading?
  
Winning in the stock market is easy. Buy low, sell high. Of course, to do
this reliably we need to be reasonably sure that when the stock is low, it
will actually rise. Making accurate predictions of this sort is difficult and
notoriously error-prone. What we can do instead, however, is make some
predictions about the behavior of /pairs/ of stocks (or, in general, stock
portfolios). The key idea here is that certain groups of stocks will usually
be very strongly correlated, and deviations from the long-term correlation
are temporary and /mean-reverting/.  For instance, consider a pair of stocks
from the same industry, e.g. Intel and Microsoft, or Ford and GM. Both pairs
will generally trend with the overall market, and with the market sector.
Within the pairs, the
correlation is not perfect: on a particular day, stock A might, relatively
speaking, outperform stock B and vice versa. We assume that /on average/,
though, the returns on these stocks are going to be linearly related.  

How do we use this to trade?  Suppose that we somehow knew that over a 30 day
period, the two stocks will generate exactly the same returns, regardless of
what the overall market is doing. Let's say that they start at the same price
(and, by assumption, they will end at the same price) -- but, by day 15,
A is trading way higher than B. If we believe in the
assumption that the returns will equilibrate by day 30, we are going to /buy/
stock B and /short/ stock A. If the assumption is correct, we will have
netted a profit of ~(F-B)+(A-F)=A-B~, where =F= gives the (identical) final
price of the two stocks, and =A= and =B= are the prices when we open our
position. Note that it doesn't matter which direction the market has moved,
nor whether our stocks rose or fell. We just had to be correct about the
returns converging. 

This strategy is known as [[http://en.wikipedia.org/wiki/Pairs_trade][pairs trading]], and the general approach of using
statistics for placing (almost) sure bets on the market goes by the name
/statistical arbitrage/. 

So this is all fine and well, but do stocks really behave like that?
Let's find out! The simplest way to do so is to pick a correlated pair and
use it go construct a "market neutral" portfolio, i.e. one with expected zero
return. We then hold this portfolio over /n/ days, and look at our /actual/
returns.  This brings us to our first code block which we will evaluate in an
R session.

** Eyeballing mean reversion: the code

First, we have to prepare the workspace, load the data, and make sure we have
the available packages installed in R.  Here, we use some time series
libraries that give us convenient rolling-window filtering, as well as pretty
plots. For the data, we provide daily returns on a small group of stocks 
(contained in =workspace/sample_returns.csv.gz=). 

#+name: explore_mean_reversion
#+header: :width 800 :height 800 :units="px"
#+begin_src R :session *R-mnr* :exports both :results graphics :file mnr.png 
  need.packages <- c("zoo", "timeSeries", "xts")
  for (p in need.packages)
    if (!is.element(p, installed.packages()[, 1]))  install.packages(p)
  require("xts", quiet=T)
  require("timeSeries", quiet=T)

  rets <- read.csv(file="workspace/sample_returns.csv.gz", row.names=1)
  
  RollingBetaFit <- function(data, win=60) {
    WindowFit <- function(data.win) {
      beta.fit <- lm.fit(cbind(rep(1, win), data.win[, 2]), data.win[, 1])
      beta.fit$coefficients[2]
    }
    betas <- rollapply(data, win, WindowFit, by.column=F)
    length(betas) <- nrow(data)
    cbind(data, beta=betas)
  }
  
  MarketNeutralReturns <- function(data, holding.period=1, timespec="/") {
    data <- data[complete.cases(data), ]
    data <- as.xts(as.timeSeries(data))[timespec]   # automatically sorts
    dates.seq <- holding.period:nrow(data)
    CompoundReturns <- function(x) exp(sum(log(1 + x))) - 1
    comp.rets <- rollapply(data[, 1:2], holding.period, CompoundReturns, by.column=T, align="left")
    mn.rets <- as.xts(timeSeries(rep(NA, nrow(comp.rets)), index(comp.rets)))
    for (i in 1:length(mn.rets))
      mn.rets[i] <- sum(comp.rets[i] * c(1, -data[i, "beta"]))
    names(mn.rets) <- paste(holding.period,"-day ret",sep="")
    mn.rets
  }
  
  PlotMNReturns <- function(rets, pair=c("JPM", "XLF"),
                            periods=c(1, 5, 15, 30), timeframe="2006/2007") {
    rets.betas <- RollingBetaFit(rets[pair])
    oldpar <- par(no.readonly=T)
    par(mfrow=c(length(periods), 1))
    for(p in periods)
      plot(MarketNeutralReturns(rets.betas, holding.period=p, timespec=timeframe),
           main=paste("Market-neutral returns:", paste(pair, collapse="/"), "held for", p, "day(s)"))
    par(oldpar)
  }
  
  PlotMNReturns(rets)
#+end_src  

#+results: explore_mean_reversion
[[file:mnr.png]]

What's happening here is the following: we pick daily returns on a pair of
stocks, and for every day look back over a 60 day window and use =lm.fit= to
get the correlation coefficient \beta.  We then construct a portfolio where
we allocate $1 to the first stock of the pair, and -$\beta to the second
stock (i.e. we are long the first stock, short the second). We then hold that
portfolio over =p= days and see what returns we generate.  At the end, we
examine a plot of these returns for the pair -- in this case, we pick JPM and
its corresponding sector ETF (XLF).

Indeed, we see a random signal that seems to oscillate around 0, and the
characteristic oscillation period increases as we increase the holding period
of the portfolio. You might be curious to know whether this mean-reverting
behavior persists if we pick a pair of stocks that we don't expect to be very
strongly correlated, e.g. JPM and MSFT, or JPM and INTC, or JPM and AA.  If
you are running this code interactively, it is worth re-running
=PlotMNReturns= with these stocks as the pair.

What you might find is that the empirical behavior that we glean from the
plots is not very consistent.  Mean reversion seems to be much better defined
for some stocks than for others, but as with all stochastic signals,
eyeballing their behavior does not get us very far. Instead, we need a more
thorough mathematical framework in which to treat the mean reversion.

* Mean-reversion mathematics

Analyzing financial time series can quickly degenerate into impenetrable
stochastic calculus and an alphabet soup denoting the various flavors of
autoregressive models. Fortunately, what we are doing here is quite simple,
and the basic model can be treated as a black-box abstraction. 

Here are a [[notes/avellaneda-lee-paper-notes.pdf][few pages of my notes]] summarizing the basic model we use for
pairs trading. The key ideas are the following:

- [[http://quantivity.wordpress.com/2011/02/21/why-log-returns/][log-returns]] for the two instruments are linearly related, with an
  additional term given by the stationary and mean-reverting process X_t
- X_t is modeled as an [[http://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process][Ornsteinâ€“Uhlenbeck process]]
- the solution to the O-U stochastic differential equation is exactly the   
  [[http://en.wikipedia.org/wiki/Autoregressive_model][AR(1)]] time series model (which we can easily fit with R)
- O-U process is characterized by a few parameters, including the speed of
  mean reversion, its mean, and the stationary (long-term) variance. We can
  extract those parameters from the AR(1) model fit.

Finally, the trading signal /s/ is just the normalized deviation of the
estimated O-U process X_t from its estimated mean, where we use the long-term
variance for normalization.  Generating the signal appears straightforward
and, indeed, it is!

* Generating the trading signals
** Working with the supplied code / preparing the environment

To manage project paths, we rely on the global variable =statarb-al.proj=,
which is must be a list containing the elements =src.path= and
=workspace.path=.  We define it as follows: suppose your project root is

: "~/finance/research/statarb-al/"

We then define =statarb-al.proj= as follows:

#+name: setup_env
#+begin_src R :exports code
  statarb.al.projectroot <- "~/finance/research/statarb-al/"
  statarb.al.proj <-
    list(src.path=paste(statarb.al.projectroot, "src/", sep=""),
         workspace.path=paste(statarb.al.projectroot, "workspace/", sep=""))
#+end_src

This bit of code must be sourced into every R session for the project. The
boilerplate code for sourcing the necessary function definitions and setting
the work directory then becomes something like

#+begin_src R :exports code
  if (!exists("statarb.al.proj")) stop("Need project metadata file to proceed")
  source.files <- c("functions.R")
  source(paste(statarb.al.proj$src.path, source.files, sep=""))
  setwd(statarb.al.proj$workspace.path)
#+end_src

We provide the following data files:

- univ1_ret_mtx.gz ([[http://dnquark.com/org/statarb/files/univ1_ret_mtx.gz][link]], 20 MB)
- etf_ret_mtx.gz ([[http://dnquark.com/org/statarb/files/etf_ret_mtx.gz][link]], 2 MB)
- ticker_to_sec_etf.csv (in the git repository)

The returns data files should be placed in the /workspace/ directory of the
project tree.  The /univ1/ file contains daily returns for several hundred
stocks; the /etf/ file contains daily returns for sector ETFs, and
/ticker_to_sec_etf.csv/ is used to associate stocks with sector ETFs using
the GICS industry classification.

Finally, now is a good time to install all the packages that this project
depends on.  They include:

: timeSeries, xts, abind, foreach, doMC, stinepack

These packages aren't crucial for the analysis itself, but time series
libraries make the presentation/handling of price data somewhat more
convenient, while =foreach= and =doMC= are used to parallelize computations on a
multicore workstation. All of the above packages can be installed
automatically using

#+begin_src R :exports code
  needs.packages(c("timeSeries", "xts", "abind", "foreach", "doMC", "stinepack"))
  for (p in need.packages)
    if (!is.element(p, installed.packages()[, 1]))  install.packages(p)
#+end_src
 

** Signal generation test: JPM vs XLF

To illustrate the general analysis workflow, we first compute the s-score for
a simple stock/ETF pair. We pick JPM and XLF as the stock and ETF.

This code also illustrates the boilerplate environment setup and data
loading. To run it, make sure the project metadata variable =statarb.al.proj=
exists in the workspace and source [[file:src/jpm_xlf_s_score.R]]

To run this from within org-mode, do:

#+call: setup_env[:session *R-jpm-test*]() :results silent

#+begin_src R :session *R-jpm-test*  :results silent
  source(paste(statarb.al.proj$src.path, "jpm_xlf_s_score.R", sep=""))
#+end_src

** Run the signal generation for all financials

The code that we wrote to test the JPM vs XLF signal was very general; the
only difference is that we now want to subset =ret.s= by all of the financial
tickers (which are given by =tc.xlf$TIC=).  Also, now that we are running the
signal generation over multiple stocks, it's a good idea to set
=subtract.average= to =T= (since this is reported to produce better
results). (In the future, it might be worth exploring whether or not that
claim is true, and to what extent.)

#+begin_src R :session *R-jpm-test*  :results silent
  ret.s.fin <- ret.s[, tc.xlf$TIC, drop=F]
  system.time(sig.fin <-
              stock.etf.signals(ret.s.fin, ret.e, tc.xlf,
                                num.days=N-est.win+1, compact.output=T, subtract.average=T))
#+end_src

This took just under a minute to run on a quad-core machine.

So this is it! If you so desire, you can generate the trading signals for all
the stocks in the dataset.  In parts 2 and 3 of this write-up, we will be
simulating the trades using the beautiful Rcpp framework, and exploring how
to use PCA analysis in constructing mean-reverting portfolios.  Stay tuned!

** Old sig.gen notes 						   :noexport:
*** Current format: list with 
   (1) list of dates and 
   (2) list of dates with a signal matrix attached
   Signal generation is performed via a command like
   sig.list.04.05 <- stock.etf.signals(ret.s,ret.e,tickers.classified,num.days=num.days,compact.output=TRUE)
   the compact.output=T is necessary to avoid a (giant) overhead of named
   attributes
#+BEGIN_SRC R
  ## compact output format:
  ## matrix with rows corresponding to stocks; each row is an unnamed numeric array A
  ## int2logical(A[1],5) gives logical w/ names corr to
  ## c("model.valid", "bto", "sto", "close.short", "close.long")
  ## A[2:8] are mr.params, names c("s","k","m","mbar","a","b","varz")
  ## A[9...] are betas (determined from stock names)
  ## For date i and ticker j, extract parameters from the matrix via something like
  sig <- decode.signals(signals[[i]][j,])
  params <- decode.params(signals[[i]][j,])
  betas <- decode.betas(signals[[i]][j,])
#+END_SRC 

*** batch-mode signal generation:
    see tr_test_spx1_batch.R
    Can call from the command line using
    RCmd tr_test_spx1_batch.r -saveSigFile TRUE -filename sig.spx1.RObj
    use -offsetYear 2005 -yearsBack 3 switch to generate selectively


* Results synopsis:
  "universe" data set (SPX + Russell 1000 + half R2K)
** ETF trading (2001-2009)
   [[file:plots/simtrade_etf1.png]]
   [[file:plots/simtrade_etf2.png]]
** PCA trading (2001-2009)   
   [[file:plots/spx_pnl_gamut_pca.png]]
** Studying eigenportfolios:
   This shows that the leading eigenvector's portfolio is a proxy for the
   market performance:
   [[file:plots/spy_vs_market_eigenportf.png]]
** Sample trading signal:
   [[file:plots/jpm.sig.png]]
** Mean reversion demonstration: PNL of a market-neutral portfolio held over n days
   Actual data: [[file:plots/MNPlotsSTT.png]]
   GARCH(1,1)/student-t simulation: [[file:plots/MNPlotsSim.png]]
** Price process simulation: parameters estimated from XLF data
   GARCH(1,1): [[file:plots/XLF_sim_garch_ar-t.png]]
   AR(2)/GARCH(1,1): [[file:plots/XLF_sim_garch_ar-garch.png]]
* Project metadata / code organization 				   :noexport:	
Project [[file:src][source tree]]  
Note that sources live in a separate location from the [[file:workspace][workspace]]

Most of the project code is written in R; trading simulation is done in C++
using RCpp; small data munging pieces are written in Perl and Python; simple
shell commands (e.g. =cut=) are used in some places for extracting fields in
CSV files.

One problem that's endemic to most R-based projects is code organization.
=source= statements are rather inconvenient for several reasons; one reason
becomes obvious when the workspace is separate from the source tree, since
the =source= command now requires a fully-qualified path.  To avoid
hard-coding a full path to project files in every =source= statement, we
choose the following compromise: every script that depends on sourcing must
look for =statarb-al.proj= file in the current workspace, like so:
#+begin_src R
  if (!exists("statarb.al.proj")) stop("Need project metadata file to proceed")
#+end_src

In turn =statarb-al.proj= must be a list containing the variable =src.path=
(and, optionally, =workspace.path=).

#+name: setup_env
#+begin_src R
  statarb.al.proj <-
    list(src.path="/home/leo/projects/finance/research/statarb-al/src/",
         workspace.path="/home/leo/projects/finance/research/statarb-al/workspace/")
#+end_src

This code can be evaluated in the relevant R session; we also put it inside
the workspace directory as =setup_env.R=.

* Tools and software used

Most of the analysis code used in the project is written in R.  The following
packages are required:
: timeSeries, xts, abind, foreach, doMC, stinepack

These packages aren't crucial to the analysis itself, but time series
libraries make the presentation/handling of price data somewhat more
convenient, while =foreach= and =doMC= are used to parallelize computations on a
multicore workstation. All of the above packages can be installed
automatically using
#+begin_src R
  install.packages(c("timeSeries", "xts", "abind", "foreach", "doMC", "stinepack"))
#+end_src

The trading simulation is done in C++ using the [[http://cran.r-project.org/web/packages/Rcpp/index.html][RCpp framework]]; small data
munging pieces are written in Perl and Python; simple shell commands
(e.g. =cut=) are used in some places for extracting fields in CSV files.

The code was developed on a Linux workstation and should be reproducible in
all Linux environments, as well as in MacOS.

* Source data
** Data organization  
    
Link: [[file:data][project data directory]]

Data directory is organized as follows:
 - [[file:data/symbols][symbols]] contains information about the instrument universe and their stock
   ticker symbols
 - [[file:data/misc_info][misc_info]] contains various useful information (e.g. about trading hours)
 - [[file:data/sources][sources]] has symlinks to archived historical price data for analysis

** Data sources  
    
For historical data, we use daily closing prices from the CRSP dataset.

See CRSP documentation (quick intro [[file:data/documentation/CRSP_intro.pdf][here]]) for the explanation of the fields.

The CRSP data for this project contains ~2000 stocks and ~600 ETFs.

Stock universe is composed of stocks listed on S&P 500 as of November 2008,
as well as the Jun 2001 Russell 1000 and the first half[fn:1] of Jun 2001 Russell 2000
lists for a grand total of 2079 ticker symbols[fn:2].  

[fn:1] We pick the half with the higher market cap.
[fn:2] The total is less than 2500 because we keep only the tickers that
exist throughout the time range of interest, i.e. ~ 1997 - 2009.

*** Preparing the current dataset 				   :noexport:
    
All the original CRSP data is in
[[file:data/sources/CSRP][file:~/projects/finance/research/statarb-al/data/sources/CSRP]]

We use the CRSP-0796-0609_spx_r21.csv.gz file, primarily.
Alias it to univ1_data_full.csv:
# ln -s ../data/sources/CRSP/CRSP-0796-0609_spx_r21.csv.gz workspace/univ1_data_full.csv.gz

Get field numbers via:

[[shell:zcat%20workspace/univ1_data_full.csv.gz|head%20-n1|tr%20','%20'\n'|nl][shell:zcat workspace/univ1_data_full.csv.gz|head -n1|tr ',' '\n'|nl]]

Important fields are:
1-DATE; 9-TICKER; 28-RET; 29-BID; 30-ASK; 10-PERMNO; 27-VOL

NOTE: extra fields might be needed; see detailed notes for the details of the
"all fields" set

Unzip the data, keeping the DATE, TICKER, RET fields:

[[shell:zcat%20workspace/univ1_data_full.csv.gz%20|%20cut%20-d%20','%20-f%201,9,28%20>%20workspace/univ1_data_ret.csv][shell:zcat workspace/univ1_data_full.csv.gz | cut -d ',' -f 1,9,28 > workspace/univ1_data_ret.csv]]


Similarly, to get the ETF data, run this to get the CSV field numbers:
[[shell:zcat%20data/sources/CRSP/CRSP-0796-0609_etf1.csv.gz|head%20-n1|tr%20','%20'\n'|nl][shell:zcat data/sources/CRSP/CRSP-0796-0609_etf1.csv.gz|head -n1|tr ',' '\n'|nl]]

The important ones are:
1-DATE; 9-TICKER; 28-RET; 10-PERMNO; 27-VOL

Once again, we unzip keeping the DATE, TICKER, RET fields:

[[shell:zcat%20data/sources/CRSP/CRSP-0796-0609_etf1.csv.gz%20|%20cut%20-d%20','%20-f%201,9,28%20>%20workspace/etf_data_ret.csv][shell:zcat data/sources/CRSP/CRSP-0796-0609_etf1.csv.gz | cut -d ',' -f 1,9,28 > workspace/etf_data_ret.csv]]


*** Data generation details (old notes) 			   :noexport:

  CRSP-97-08.csv.gz  CRSP-97-08_etf.csv.gz  CRSP-97-08_etf_shrt.csv.gz
  Obtained from WRDS CRSP database using my reference universe (SPX tickers
  as of Nov 2008 plus a list of ETFs).
  Selected a bunch of fields, so need to filter out the relevant ones; on the
  most basic level only need TICKER, DATE, RET (period returns,
  i.e. returns with dividends correctly accounted for).
  
  Date range: 01Jul1996 - 30Jun2009
  Ticker list: [[file:~/projects/finance/spx_tickers_20081107.txt][SPX 20081107]] (~/projects/finance/spx_tickers_20081107.txt)
  ETF list: (file:~/projects/finance/ETF_master_tkrs.txt)

**** Data generation: 100108
  Variables Selected:
  DATE,CUSIP,DCLRDT,PAYDT,RCRDDT,SHRFLG,TICKER,PERMNO,EXCHCD,NAICS,PRIMEXCH,TRDSTAT,SECSTAT,DISTCD,DIVAMT,FACPR,FACSHR,ACPERM,ACCOMP,SHROUT,BIDLO,ASKHI,PRC,VOL,RET,BID,ASK,CFACPR,CFACSHR,OPENPRC,SXRET,BXRET,NUMTRD,RETX,vwretd,vwretx,ewretd,ewretx,sprtrn
  Here we number them in emacs (via number-lines-region); 1-DATE; 7-TICKER; 25-RET; 
  :DETAILS:
 1. DATE
 2. CUSIP
 3. DCLRDT
 4. PAYDT
 5. RCRDDT
 6. SHRFLG
 7. TICKER
 8. PERMNO
 9. EXCHCD
10. NAICS
11. PRIMEXCH
12. TRDSTAT
13. SECSTAT
14. DISTCD
15. DIVAMT
16. FACPR
17. FACSHR
18. ACPERM
19. ACCOMP
20. SHROUT
21. BIDLO
22. ASKHI
23. PRC
24. VOL
25. RET
26. BID
27. ASK
28. CFACPR
29. CFACSHR
30. OPENPRC
31. SXRET
32. BXRET
33. NUMTRD
34. RETX
35. vwretd
36. vwretx
37. ewretd
38. ewretx
39. sprtrn
:END:

  #  Isolate the DATE, TICKER, RET fields via
  cut -d ',' -f 1,7,25 spx_data_full.csv > spx_data_ret.csv
  cut -d ',' -f 1,7,25 etf_data_full.csv > etf_data_ret.csv
  # 1669379 spx_data_ret.csv # num recs
  # 655064 etf_data_ret.csv  # num recs

**** Data generation: "universe 1" (10/01/09)
     Decided to expand the stock universe by merging the Nov 2008 SPX ticker
     list with the Jun 2001 Russell 1000 and first half of Jun 2001 Russell
     2000 lists for a grand total of 2079 ticker symbols.  Also selected
     additional variables, among which is PERMCO to keep track of ticker
     renamings, etc
     Number them in emacs (via number-lines-region): 
     1-DATE; 9-TICKER; 28-RET; 29-BID; 30-ASK
     10-PERMNO; 27-VOL
     Call the dataset "universe 1"
     :DETAILS:
   1. DATE
   2. HSICMG
   3. HSICIG
   4. CUSIP
   5. DCLRDT
   6. PAYDT
   7. RCRDDT
   8. SHRFLG
   9. TICKER
   10. PERMNO
   11. EXCHCD
   12. NAICS
   13. PRIMEXCH
   14. TRDSTAT
   15. SECSTAT
   16. PERMCO
   17. DISTCD
   18. DIVAMT
   19. FACPR
   20. FACSHR
   21. ACPERM
   22. ACCOMP
   23. SHROUT
   24. BIDLO
   25. ASKHI
   26. PRC
   27. VOL
   28. RET
   29. BID
   30. ASK
   31. CFACPR
   32. CFACSHR
   33. OPENPRC
   34. SXRET
   35. BXRET
   36. NUMTRD
   37. RETX
   38. vwretd
   39. vwretx
   40. ewretd
   41. ewretx
   42. sprtrn
     :END:
     #  Isolate the DATE, TICKER, RET fields via
     cut -d ',' -f 1,9,28 univ1_data_full.csv > univ1_data_ret.csv
**** Data generation: all fields (10/01/11)
   Realized needed the full GICS code field which wasn't selected, so reran
   query for universe1 stocks with all fields selected.  All fields are
   1-DATE; 18-TICKER; 50-RET; 
   19-PERMNO; 49-VOL; 51-BID; 52-ASK
   10-HSICCD; 2-HSICMG; 3-HSICIG
   #  Isolate the DATE, TICKER, RET, PERMNO, VOL, HSI.. fields via
   cut -d ',' -f 1,18,50,10,2,3 spx_data_full_allf.csv > univ1_data_xtrafields.csv

   :DETAILS:
 1. DATE
 2. HSICMG
 3. HSICIG
 4. COMNAM
 5. CUSIP
 6. DCLRDT
 7. DLAMT
 8. DLPDT
 9. DLSTCD
10. HSICCD
11. ISSUNO
12. NCUSIP
13. NEXTDT
14. PAYDT
15. RCRDDT
16. SHRCLS
17. SHRFLG
18. TICKER
19. PERMNO
20. NAMEENDT
21. SHRCD
22. EXCHCD
23. SICCD
24. TSYMBOL
25. NAICS
26. PRIMEXCH
27. TRDSTAT
28. SECSTAT
29. PERMCO
30. HEXCD
31. DISTCD
32. DIVAMT
33. FACPR
34. FACSHR
35. ACPERM
36. ACCOMP
37. NWPERM
38. DLRETX
39. DLPRC
40. DLRET
41. SHROUT
42. TRTSCD
43. NMSIND
44. MMCNT
45. NSDINX
46. BIDLO
47. ASKHI
48. PRC
49. VOL
50. RET
51. BID
52. ASK
53. CFACPR
54. CFACSHR
55. OPENPRC
56. SXRET
57. BXRET
58. NUMTRD
59. RETX
60. vwretd
61. vwretx
62. ewretd
63. ewretx
64. sprtrn
:END:

**** ETF data as of 100109:
    Discovered that ETF data hasn't been regenerated using the latest set of
    fields/time periods; for now will stick to using it with the fields:
    1-DATE; 2-TICKER; 16-RET
    #  Isolate the DATE, TICKER, RET fields via
    cut -d ',' -f 1,2,16 etf_data_full.csv > etf_data_ret.csv
    :DETAILS:
 1. DATE
 2. TICKER
 3. PERMNO
 4. EXCHCD
 5. TRDSTAT
 6. SECSTAT
 7. DISTCD
 8. DIVAMT
 9. FACPR
10. FACSHR
11. SHROUT
12. BIDLO
13. ASKHI
14. PRC
15. VOL
16. RET
17. OPENPRC
18. SXRET
19. BXRET
20. NUMTRD
21. RETX
    :END:
**** ETF data as of 100110:
     Regenerated the ETF data, fields are (like in the latest stock data)
      1-DATE; 9-TICKER; 28-RET; 
     10-PERMNO; 27-VOL
     cut -d ',' -f 1,9,28 etf1_data_full.csv > etf_data_ret.csv
**** List of dates available in file dates_vec_090630
     when the full spx matrix was still loaded, did
     dates.vector <- as.numeric(row.names(spx.ret.mtx.full))
     write.csv(dates.vector,file="dates_vec_090630",row.names=F)
**** 15 ETFs from Table 3 and Table 4 of the paper:
    :DETAILS:
HHH
IYR
IYT
OIH
RKH
RTH
SMH
UTH
XLE
XLF
XLI
XLK
XLP
XLV
XLY
    :END:

** Data preprocessing     
*** Assign sector ETFs to stocks 
 
In the first part of this project, we explore mean-reverting pair trading
between a stock and an ETF from the same sector. Following Avellaneda and
Lee, we pick 15 sector ETFs and associate each stock with an ETF using its
[[http://en.wikipedia.org/wiki/Global_Industry_Classification_Standard][GICS]] classification number.

We use the following correspondence between the GICS numbers and our list of
sector ETFs:

| ETF | Sector                 |          GICS information |
|-----+------------------------+---------------------------|
| HHH | Internet               |                    451010 |
| IYR | Real Estate            |                      4040 |
| IYT | Transportation         |                      2030 |
| OIH | Oil Exploration        |                    101020 |
| RKH | Regional Banks         |                  40101015 |
| RTH | Retail                 |                      2550 |
| SMH | Semi                   |                      4530 |
| UTH | Utilities              |                        55 |
| XLE | Energy                 |            10 excl 101020 |
| XLF | Financials             | 40 excl 4040 and 40101015 |
| XLI | Industrials            |              20 excl 2030 |
| XLI | Materials              |                        15 |
| XLK | Technology             |   45 excl 451010 and 4530 |
| XLK | Telecom                |                        50 |
| XLP | Consumer Staples       |                        30 |
| XLV | Healthcare             |                        35 |
| XLY | Consumer Discretionary |              25 excl 2550 |

Note: we combine materials (15) with industrials (20); telecom (50) with
technology (45), assigning both of the former to XLI and the latter to XLK

In the CRSP data, the GICS numbers are given by the following fields:
=GSECTOR, GGROUP, GIND, GSUBIND=
    
These give classifications of increasing specificity.  For instance, a
security might have a classification of 40101015, where the numbers 40, 4010,
401010, and 40101015 give the =GSECTOR, GGROUP, GIND, GSUBIND= fields, respectively.

To determine the industry information, we retrieve these fields from CRSP for
each symbol in our stock universe. 
:DETAILS:
The results are stored in [[file+emacs:data/symbols/ticker_to_classifiations.csv][ticker_to_classification.csv]]
The fields retrieved are:
gvkey,conm,tic,sic,naics,linkprim,liid,linktype,linkid,lpermno,lpermco,USEDFLAG,linkdt,linkenddt,GSECTOR,GGROUP,GIND,GSUBIND,SPCINDCD,SPCSECCD
To assign the stock/ETF correspondence based on this table above, we use the
following quick-and-dirty Perl script:
[[file:src/get_sector_etfs.pl][get_sector_etfs.pl]]
:END:

The following gives us the stock/ETF correspondence, as well as a quick tally
of the stocks assigned to a particular ETF.  At the end, about 3/4 of the
stocks in our universe end up paired with an ETF.

#+begin_src sh :exports both
  src/get_sector_etfs.pl < data/symbols/ticker_to_classifiations.csv | uniq > workspace/ticker_to_sec_etf.csv
  cd workspace
  for etf in HHH IYR IYT OIH RKH RTH SMH UTH XLE XLF XLI XLK XLP XLV XLY; do 
      echo $(grep $etf ticker_to_sec_etf.csv |wc -l) $etf;
  done
  cut -d',' -f1 ticker_to_sec_etf.csv|sed '1d' > tickers_classified
  wc -l tickers_classified 
#+end_src

#+results:
|   37 | HHH                |
|   76 | IYR                |
|   37 | IYT                |
|   49 | OIH                |
|   76 | RKH                |
|   71 | RTH                |
|   83 | SMH                |
|   78 | UTH                |
|   38 | XLE                |
|  158 | XLF                |
|  244 | XLI                |
|  273 | XLK                |
|   75 | XLP                |
|  216 | XLV                |
|  185 | XLY                |
| 1696 | tickers_classified |

*** Generate returns matrices

The DATE, TICKER, and RET fields were pulled out of the original CRSP
datasets for the stocks and ETFs.  To convert them to returns matrices sorted
by date and ticker, we run a python script =convert.py= (originally due to
J. Michael Steele at Wharton).  This script takes the DATE, TICKER, and RET
lines and puts returns in a matrix form; the output is a csv file with fields
DATE,TICKER1,TICKER2,...,TICKERn, with lines giving returns grouped by date.

#+name: ret_mtx_etfs
: etf_ret_mtx

#+name: ret_mtx_stocks
: univ1_ret_mtx

#+begin_src sh :exports code :noweb yes
  # this shold take about 5 minutes to run
  cd workspace
  ../src/lib/convert.py -i etf_data_ret.csv -o <<ret_mtx_etfs>>
  ../src/lib/convert.py -i univ1_data_ret.csv -o <<ret_mtx_stocks>>
#+end_src

*** Old notes 							   :noexport:
    Convert to a returns matrix sorted by date, ticker:
    # ./convert.py -i etf_data_ret.csv -o etf_old_ret_mtx
    ./convert.py -i etf_data_ret.csv -o etf_ret_mtx
    ./convert.py -i spx_data_ret.csv -o spx_ret_mtx
    ./convert.py -i univ1_data_ret.csv -o univ1_ret_mtx
    
    Correlation matrix: get rid of the tickers that have too many NAs
    proc_corr.R
    
    Issues with NAs: filtering out the spx_ret_mtx to the point where we have
    no NAs brings us down from 682 to 412 names.  Most NAs seem to come from
    things like ticker change due to mergers, etc -- so a better solution is,
    perhaps, to use the PERMNO (not TICKER).  Raising "no NA" threshold from 0
    to something small (a few percent) doesn't result in a substantial
    increase in ticker symbols (10% cutoff gets us 440, 30 extra names), so
    for simplicity it might be worth keeping it at zero.
* Backtesting
** Trading simulation flow 					   :noexport:
  select stocks to trade against ETFs/synthetic ETFs
  pre-generate signals
  go through dates in chronological order
  for every stock, examine signals
  Note that because the short-to-open/buy-to-close and
  buy-to-open/sell-to-close signals form bands above and below zero
  respectively, we are either short or long, never both.

  *We also need to filter the beta-portfolio:*
   - eliminate values that are less than B.THR percent of that maximum
     component in absolute value
   - eliminate negative values

** Trading process pseudocode:
 for every day: for every stock:
  if model.valid:
    if STO:
      if(!short): #flat or long (but shouldn't be long here)
	sell stock, buy factors #opening short (if flat before, as we should be)
	if(long): warning("STO tripped while long, weird jump?")
      else: do nothing #already short
    if CLOSE.SHORT:
      if(short): 
	buy stock, sell factors #closing short
	else: do nothing
    if BTO:
      if(!long): #flat or short (but shouldn't be short here)
        buy stock, sell factors #opening long
	if(short): warning("BTO tripped while short, weird jump?")
      else: do nothing #already long
    if CLOSE.LONG:
      if(long):
        sell stock, buy factors #closing long
      else: do nothing
** Determining transaction quantities
   We scale the investments in proportional to the current equity:
   Q[t] = Equity[t]*Lambda[t], where lambda is determined by the desired
   leverage (e.g. if expecting 100 long/100 short portfolio with 2+2
   leverage, lambda=2/100; cf page 22 of AL paper)
   For every stock and beta-portfolio component, we compute Q[t]/price,
   round, and get the number of shares.
** Trading setup:
   first, we need to create price tables from data
   for now, just use the convert python script with bid/asks instead of rets
   1-DATE; 9-TICKER; 28-RET; 29-BID; 30-ASK
*** shell commands to generate price tables
   cut -d ',' -f 1,9,29 univ1_data_full.csv > univ1_data_bid.csv
   cut -d ',' -f 1,9,30 univ1_data_full.csv > univ1_data_ask.csv
   ./convert-bid.py -i univ1_data_bid.csv -o univ1_bid_mtx
   ./convert-ask.py -i univ1_data_ask.csv -o univ1_ask_mtx
   cut -d ',' -f 1,9,29 etf1_data_full.csv > etf_data_bid.csv
   cut -d ',' -f 1,9,30 etf1_data_full.csv > etf_data_ask.csv
   ./convert-bid.py -i etf_data_bid.csv -o etf_bid_mtx
   ./convert-ask.py -i etf_data_ask.csv -o etf_ask_mtx
*** Work with mid-prices; here's the code to generate master mid-price tables
    test.ask <- get.mtx.gen("etf_ask_mtx",M=9*252,offset=offset.2009,file=TRUE)
    test.bid <- get.mtx.gen("etf_bid_mtx",M=9*252,offset=offset.2009,file=TRUE)
    stocks.mid.price <- (test.ask+test.bid)/2

    test.ask <- get.mtx.gen("etf_ask_mtx",M=9*252,offset=offset.2009,file=TRUE)
    test.bid <- get.mtx.gen("etf_bid_mtx",M=9*252,offset=offset.2009,file=TRUE)
    etf.mid.price <- (test.ask+test.bid)/2

    univ1.master.price <- cbind(etf.mid.price,stocks.mid.price)
   
** R issues with signal generation
   Data structures in R are extremely wasteful if you liberally use lists
   with mixed types and named objects.  This probably slows down the whole
   calculation significantly.  Temporary fix is to compact all the generated
   signals for a given date into a matrix; size is about 800K/400 stocks/25
   days
** Data offsets (assuming R data frames are reverse-chronologically sorted)
   This assumes data sets ending on 20090630
   which(as.logical(match(dates.vector,20090102)))
   ## 124
   which(as.logical(match(dates.vector,20080102)))
   ## 377
   which(as.logical(match(dates.vector,20070103)))
   ##  628
   which(as.logical(match(dates.vector,20060103)))
   ##  879
   which(as.logical(match(dates.vector,20050103)))
   ## 1131
   which(as.logical(match(dates.vector,20040102)))
   ## 1383
   which(as.logical(match(dates.vector,20030102)))
   ##  1635

   offset.2009 <- 124
   offset.2008 <- 377
   offset.2007 <- 628
   offset.2006 <- 879
   offset.2005 <- 1131
   offset.2004 <- 1383
   offset.2003 <- 1635
   
* Debugging backtesting
** First, need to ascertain that the returns datasets and the prices datasets are consistent
   -> did a spot check on XLF and JPM, the computed logreturns, returns, and
      reported returns are all consistent
** Isolated pair trading sequence: JPM and XLF -- examine the signals
   (Using 04-05 data)
   First signal:
56  56 pos: 0 ,inv.targ: 1000 ratio  0.80809  prices:  41.005 28.925  num shares:  103 -180 
BTO: 'acquiring' 103 -180  paying  -982.985 
beta.56 <- 1.237
What do we expect to happen if beta remains constant:
assuming alpha is negligible relative to mean-reverting contribution, we
   predict JPM prices from beta and XLF prices; the true price by the time
   the sell signal trips is expected to be higher due to positive increment
   in the mean-reverting Xt process.
88  88 pos: 103 ,inv.targ: 997.9351 ratio  0.7612402  prices:  36.475 27.48  num shares:  87 -152 
CLOSING LONG: paying  1189.475 
  Cash inflow is negative, so something went wrong
To examine the signals, take the debug output, save it to a file and extract
   the fields via somn like
 perl -lane 'print "$F[7],$F[9],$F[10]"' jpm.xlf.tmp > jpm.xlf.dbg1 ##OR:
 perl -lane 'print "$F[6],$F[8],$F[9]"' jpm.xlf.tmp > jpm.xlf.dbg1
** Simulation:
   simulating the mean reversion in R
   AR(1) process: use the filter function
   'y[i] = x[i] + f[1]*y[i-1] + ... + f[p]*y[i-p]'
   Command is something like
   wn <- rnorm(N)  ## (white noise)
   ar1 <- filter(wn,filter=c(.2),method="recursive")
** saved signals:
   sig.financials2.RObj  
   Tickers (not all have classification, so intersect the below list with classified$TIC):
   "ACAS" "AFL"  "AIG"  "ALL"  "AOC"  "AXP"  "BAC"  "BEN"  "BK"   "C"   
   "CB"   "CINF" "CIT"  "CMA"  "CME"  "COF"  "FII"  "GS"   "HCBK" "HIG" 
   "JPM"  "LM"   "LNC"  "LUK"  "MBI"  "MCO"  "MER"  "MET"  "MMC"  "NTRS"
   "PFG"  "PGR"  "PRU"  "SLM"  "STT"  "TMK"  "TROW" "UNM"  "USB"  "WB"  
   "WFC"  "XL"  
   Dates: 20030326 - 10071231
   sig.spx2NI.RObj
* Simulation for the "universe" stocks:
time ./sigGen.sh univ1_ret_mtx sig.univ1.RObj
# Warning messages:
# 1: In log(x$ar) : NaNs produced
# ...
# real    180m17.269s
  analysis in tr_test_univ1.R

Check for data NA runs that could be problematic in a simulation:
## study if we have any abnormally long NA runs other than the initial "instrument doesn't exist" scenario
sig.mtx.na <- apply(sig.mtx.f,c(1,3),function(x) any(is.na(x)))
sig.mtx.na.rle <- apply(sig.mtx.na,2,function(z)rev(sort(rle(unname(z))$lengths[rle(unname(z))$values])))
sig.mtx.na.len <- lapply(sig.mtx.na.rle,length)
head(rev(sort(unlist(sig.mtx.na.len))))
# FARM   ZLC   XTO   XOM WTSLA   WSM 
#    3     1     1     1     1     1 
# looks OK; and note that if we only have 1 entry that probably results from 
# NA at the beginning of the data period for instruments with non-existing ETF
Let's check if we have long runs of "model invalid" flags:

sig.mtx.modinv <- apply(sig.mtx.f,c(1,3),function(x){ mv <-decode.signals(x[1])[1]; (!mv || is.na(mv)) })
sig.mtx.modinv.rle <- apply(sig.mtx.modinv,2,function(z)rev(sort(rle(unname(z))$lengths[rle(unname(z))$values])))
sig.mtx.modinv.len <- lapply(sig.mtx.modinv.rle,length)
head(rev(sort(unlist(sig.mtx.modinv.len))))
# ED  RLI MATK  DBD CPWM  COG 
# 10    9    7    7    7    7 
# assume it's OK for now, but it would be valuable to define a limit on max
# NA run
Note that the NA in action field currently occurs where k is NaN (looks like
due to neg. AR coeff)
* Converting trading to C++
** Variables passed:
   instr.p, instr.q, dates: as Rcpp::CharacterVector
   pq.classifier as: Rcpp::CharacterVector
   prices, positions as: Rcpp::NumericVector
   sig.mtx as: Rcpp::NumericVector
   sig.actions as: Rcpp::NumericVector

   additional variables needed to create instr.p/tickers and instr.pq/names(prices) correspondence.
   NB: for the purposes of the trading loop instr.q and instr.pq are used interchangeably, both 
   mean union of P and Q
   prices.instrpq.idx, tickers.instrp.idx as: Rcpp::NumericVector
   
   function call:
   backtest_loop(instr.p, tickers.instrp.idx, instr.q, prices.instrpq.idx, dates, pq.factor.list, prices,
   positions, sig.mtx, sig.actions, params)
* Trading simulation results:
 load("univ.trading.sim.cpp.res.RObj")
 load("univ.trading.sim.cpp.res.sub.RObj")
 x11(); plot(sim.trades.f.all.cpp$equity,type='l')
 x11(); plot(sim.trades.f.all.cpp.subtr$equity,type='l')
Also see "univ.trading.sim.bugged.cpp.RObj" for the broken simulation output
* Parallelizing the computation/timing experiments:
** Timing experiments
Running for N=300 on univ1 dataset
code with lists: master branch sha 207b620407b4d35366900e1847ec5f82bbf5bd8d
real    14m23.385s
user    14m23.120s
sys     0m0.240s
Now code which passes everything through global matrices:
real    7m59.014s
user    7m58.760s
sys     0m0.280s
The whole global assignment issue doesn't save you much, it turns out --
timings with pass-by-value semantics: (currently in the temp branch)
real    7m56.796s
user    7m56.580s
sys     0m0.200s
using %dopar% in the gen.pq: doesn't get you much
real    7m8.989s
user    15m40.420s
sys     5m31.950s
using %dopar% on the "over stocks" loop
Code with global assignments:
real    2m33.042s
user    10m22.130s
sys     0m2.080s
Code with call-by-value matrix passing / return 
commit 99555b8d6de944ab0352ff981d5647b8cd859edc (nb: "magic numbers" in
gen.fit.pq here)
real    2m32.712s
user    11m59.090s
sys     0m2.060s
Finally, parallelize the last bit (computing S from beta, ar):
real    2m31.911s
user    13m54.930s
sys     0m3.790s
doesn't look like we save much if at all
------
The problem with the foreach/dopar timings in the above parallelization was
the lack of .multicombine=TRUE statement, leading to much overhead.  This
explains the following result:
Running on the whole set (both loops parallelized):
leo@matroskin statarb-al $ time ./sigGen.sh univ1_ret_mtx sig.univ1.PAR1.RObj
real    40m2.157s
user    123m7.840s
sys     9m13.050s
Now check out the non-parallelized signal gen. version and test it and what
the heck?...:
leo@matroskin statarb-al $ time ./sigGen.sh univ1_ret_mtx sig.univ1.PAR0.RObj
real    20m10.507s
user    93m26.610s
sys     2m4.170s
Answer: lots of communication overhead when doing sequential combines in the
s-signal loop (which simply does lots of trivial algebra).  cf e.g. this
Stack Overflow post for what is probably a similar scenario:

So now, the timings with .multicombine in place.  It looks like parallelizing
the second loop gives a (small) advantage:
One foreach/dopar:
leo@matroskin statarb-al $ time ./sigGen.sh univ1_ret_mtx sig.univ1.PAR0m.RObj
real    16m39.910s
user    91m32.640s
sys     0m3.080s
Two foreach/dopars:
leo@matroskin statarb-al $ time ./sigGen.sh univ1_ret_mtx sig.univ1.PAR1m.RObj
real    14m24.365s
user    106m27.390s
sys     0m8.910s
 
** Checking consistency of the signals:
Checking that the signals produce the same trading simulation results:
first, re-run the trading on signals from old list-based code:
save(sim.trades.f.all.cpp.subtr,file="tr_sim_univ1_subtr_list.RObj")
Now do the signals without the 2nd %dopar%:
save(sim.trades.f.all.cpp.subtr,file="tr_sim_univ1_subtr_par0.RObj")
Now do the signals with the 2nd %dopar%:
save(sim.trades.f.all.cpp.subtr,file="tr_sim_univ1_subtr_par1.RObj")

load them all up and compare:
load("tr_sim_univ1_subtr_list.RObj")
sim.list.eq <- sim.trades.f.all.cpp.subtr$equity
load("tr_sim_univ1_subtr_par1.RObj")
sim.par1.eq <- sim.trades.f.all.cpp.subtr$equity
load("tr_sim_univ1_subtr_par0.RObj")
sim.par0.eq <- sim.trades.f.all.cpp.subtr$equity
all.equal(sim.list.eq, sim.par1.eq, sim.par0.eq)
## TRUE

** NB: efficiency of the combining function matters:
   An example: matrix dimensions 48 6558  408
   combining along the 3rd dimension (400 48x6558 matrixes), so about 2.5
   megs each -- took exactly 15 min via abind(...,along=3) w/o the
   multicombine flag -- 23% of the total compute time!
* Eigenportfolio / PCA approach:
  From the paper:
  order the eigenvalues of the correlation matrix and the corresponding
  eigenvectors
  The amount invested in each stock is v_i/sigma_i, where sigma_i^2 is the
  sample variance of ith stock (in the recent M-day window)
  Using that weighting it is possible to compute the returns of every
  eigenportfolio.  
  The eigenportfolio returns are then used in the fitting procedure.
  Thus, the tasks are:
  - for every stock, compute the weights of the top m eigenportfolios and the
    returns associated with each eigenportfolio
  - use these returns to fit m betas

  Additional issues: how do we interpret the effective portfolio returns?..
  Since the "position allocations" within each portfolio aren't normalized,
  we can't treat them as % returns.  
  
  In some notes, Avellaneda scales eigenreturns by sqrt(lambda) (cf slide 7
  of Lecture2Risk2010.pdf); this seems to give eigenreturns on a more uniform
  scale, so I will follow this approach.

  Currently, store the PCA results in a 3d matrix of 
  (dates (chron) x eig. stuff x stocks)
  eig. stuff is a N(m+1)+2m array, where N is num. stk, m is
  num. eigenvectors to keep
  | { v_i/sigma_i } x m | sigma_i | F_k | lambda_k |
  |      N x m          |    N    |  m  |    m     |

  - seems to be a lot of overhead in doign an abind() of these frames.  Might
    want to ask R-help about the best way of storing large datasets.

  In order to make sure I understand what is goign on with eigenportfolios,
  reproduce Figs. 3, 4, 5.

  I get good qualitative agreement, observing clustering, as well as
  positive-weighted market eigenportfolio that tracks very close to SPY.
  Note that after 2004 I don't get any negative weights in the market
  eigenportfolio; have 1 (sometimes 0) in '04, have 1-4 in '02.

  All this analysis is in pca_test_fig3.R
