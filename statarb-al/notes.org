Objective: to implement the trading algorithms and reproduce the results
(specifically, Figures 8, 9, 10, 11, and 17) described in the following
paper: 
[[file:~/projects/finance/literature/AvellanedaLeeStatArb20090616.pdf][M. Avellaneda and J.-H. Lee. Statistical Arbitrage in the U.S. Equities Market.]]

My notes on the paper: 
[[file:~/projects/finance/notes/avellaneda-lee-paper-notes.jnt][(jnt file)]]

* Source data:
  CRSP-97-08.csv.gz  CRSP-97-08_etf.csv.gz  CRSP-97-08_etf_shrt.csv.gz
  Obtained from WRDS CRSP database using my reference universe (SPX tickers
  as of Nov 2008 plus a list of ETFs).
  Selected a bunch of fields, so need to filter out the relevant ones; on the
  most basic level only need TICKER, DATE, RET (period returns,
  i.e. returns with dividends correctly accounted for).
  
  Date range: 01Jul1996 - 30Jun2009
  Ticker list: [[file:~/projects/finance/spx_tickers_20081107.txt][SPX 20081107]] (~/projects/finance/spx_tickers_20081107.txt)
  ETF list: (file:~/projects/finance/ETF_master_tkrs.txt)

** Data generation: 100108
  Variables Selected:
  DATE,CUSIP,DCLRDT,PAYDT,RCRDDT,SHRFLG,TICKER,PERMNO,EXCHCD,NAICS,PRIMEXCH,TRDSTAT,SECSTAT,DISTCD,DIVAMT,FACPR,FACSHR,ACPERM,ACCOMP,SHROUT,BIDLO,ASKHI,PRC,VOL,RET,BID,ASK,CFACPR,CFACSHR,OPENPRC,SXRET,BXRET,NUMTRD,RETX,vwretd,vwretx,ewretd,ewretx,sprtrn
  Here we number them in emacs (via number-lines-region); 1-DATE; 7-TICKER; 25-RET; 
  :DETAILS:
 1. DATE
 2. CUSIP
 3. DCLRDT
 4. PAYDT
 5. RCRDDT
 6. SHRFLG
 7. TICKER
 8. PERMNO
 9. EXCHCD
10. NAICS
11. PRIMEXCH
12. TRDSTAT
13. SECSTAT
14. DISTCD
15. DIVAMT
16. FACPR
17. FACSHR
18. ACPERM
19. ACCOMP
20. SHROUT
21. BIDLO
22. ASKHI
23. PRC
24. VOL
25. RET
26. BID
27. ASK
28. CFACPR
29. CFACSHR
30. OPENPRC
31. SXRET
32. BXRET
33. NUMTRD
34. RETX
35. vwretd
36. vwretx
37. ewretd
38. ewretx
39. sprtrn
:END:

  #  Isolate the DATE, TICKER, RET fields via
  cut -d ',' -f 1,7,25 spx_data_full.csv > spx_data_ret.csv
  cut -d ',' -f 1,7,25 etf_data_full.csv > etf_data_ret.csv
  # 1669379 spx_data_ret.csv # num recs
  # 655064 etf_data_ret.csv  # num recs

** Data generation: "universe 1" (10/01/09)
   Decided to expand the stock universe by merging the Nov 2008 SPX ticker
   list with the Jun 2001 Russell 1000 and first half of Jun 2001 Russell
   2000 lists for a grand total of 2079 ticker symbols.  Also selected
   additional variables, among which is PERMCO to keep track of ticker
   renamings, etc
   Number them in emacs (via number-lines-region): 
   1-DATE; 9-TICKER; 28-RET; 29-BID; 30-ASK
   10-PERMNO; 27-VOL
   Call the dataset "universe 1"
   :DETAILS:
 1. DATE
 2. HSICMG
 3. HSICIG
 4. CUSIP
 5. DCLRDT
 6. PAYDT
 7. RCRDDT
 8. SHRFLG
 9. TICKER
 10. PERMNO
 11. EXCHCD
 12. NAICS
 13. PRIMEXCH
 14. TRDSTAT
 15. SECSTAT
 16. PERMCO
 17. DISTCD
 18. DIVAMT
 19. FACPR
 20. FACSHR
 21. ACPERM
 22. ACCOMP
 23. SHROUT
 24. BIDLO
 25. ASKHI
 26. PRC
 27. VOL
 28. RET
 29. BID
 30. ASK
 31. CFACPR
 32. CFACSHR
 33. OPENPRC
 34. SXRET
 35. BXRET
 36. NUMTRD
 37. RETX
 38. vwretd
 39. vwretx
 40. ewretd
 41. ewretx
 42. sprtrn
   :END:
   #  Isolate the DATE, TICKER, RET fields via
   cut -d ',' -f 1,9,28 univ1_data_full.csv > univ1_data_ret.csv
** Data generation: all fields (10/01/11)
   Realized needed the full GICS code field which wasn't selected, so reran
   query for universe1 stocks with all fields selected.  All fields are
   1-DATE; 18-TICKER; 50-RET; 
   19-PERMNO; 49-VOL; 51-BID; 52-ASK
   10-HSICCD; 2-HSICMG; 3-HSICIG
   #  Isolate the DATE, TICKER, RET, PERMNO, VOL, HSI.. fields via
   cut -d ',' -f 1,18,50,10,2,3 spx_data_full_allf.csv > univ1_data_xtrafields.csv

   :DETAILS:
 1. DATE
 2. HSICMG
 3. HSICIG
 4. COMNAM
 5. CUSIP
 6. DCLRDT
 7. DLAMT
 8. DLPDT
 9. DLSTCD
10. HSICCD
11. ISSUNO
12. NCUSIP
13. NEXTDT
14. PAYDT
15. RCRDDT
16. SHRCLS
17. SHRFLG
18. TICKER
19. PERMNO
20. NAMEENDT
21. SHRCD
22. EXCHCD
23. SICCD
24. TSYMBOL
25. NAICS
26. PRIMEXCH
27. TRDSTAT
28. SECSTAT
29. PERMCO
30. HEXCD
31. DISTCD
32. DIVAMT
33. FACPR
34. FACSHR
35. ACPERM
36. ACCOMP
37. NWPERM
38. DLRETX
39. DLPRC
40. DLRET
41. SHROUT
42. TRTSCD
43. NMSIND
44. MMCNT
45. NSDINX
46. BIDLO
47. ASKHI
48. PRC
49. VOL
50. RET
51. BID
52. ASK
53. CFACPR
54. CFACSHR
55. OPENPRC
56. SXRET
57. BXRET
58. NUMTRD
59. RETX
60. vwretd
61. vwretx
62. ewretd
63. ewretx
64. sprtrn
:END:

*** ETF data as of 100109:
    Discovered that ETF data hasn't been regenerated using the latest set of
    fields/time periods; for now will stick to using it with the fields:
    1-DATE; 2-TICKER; 16-RET
    #  Isolate the DATE, TICKER, RET fields via
    cut -d ',' -f 1,2,16 etf_data_full.csv > etf_data_ret.csv
    :DETAILS:
 1. DATE
 2. TICKER
 3. PERMNO
 4. EXCHCD
 5. TRDSTAT
 6. SECSTAT
 7. DISTCD
 8. DIVAMT
 9. FACPR
10. FACSHR
11. SHROUT
12. BIDLO
13. ASKHI
14. PRC
15. VOL
16. RET
17. OPENPRC
18. SXRET
19. BXRET
20. NUMTRD
21. RETX
    :END:
*** ETF data as of 100110:
    Regenerated the ETF data, fields are (like in the latest stock data)
     1-DATE; 9-TICKER; 28-RET; 
    10-PERMNO; 27-VOL
    cut -d ',' -f 1,9,28 etf1_data_full.csv > etf_data_ret.csv
*** List of dates available in file dates_vec_090630
    when the full spx matrix was still loaded, did
    dates.vector <- as.numeric(row.names(spx.ret.mtx.full))
    write.csv(dates.vector,file="dates_vec_090630",row.names=F)
*** 15 ETFs from Table 3 and Table 4 of the paper:
    :DETAILS:
HHH
IYR
IYT
OIH
RKH
RTH
SMH
UTH
XLE
XLF
XLI
XLK
XLP
XLV
XLY
    :END:

*** Industry sectors / determining ETF correspondence:
NB: materials (15) will lump with industrials (20); telecom (50) with technology (45)
HHH, Internet:
451010
IYR, RE:
4040
IYT, transportation:
2030
OIH, oil expl:
101020
RKH, regional banks:
40101015
RTH, retail:
2550
SMH, semi:
4530
UTH, utils:
55
XLE, energy:
10 excl 101020
XLF, financials:
40 excl 4040 and 40101015
XLI, industrials:
20 excl 2030
15 (materials)
XLK, technology:
45 excl 451010 and 4530
50 (telecom)
XLP, consumer staples:
30
XLV, healthcare:
35
XLY, consumer discretionary:
25 excl 2550
    
*** industry sectors/etf correspondence: code and results
    ./get_sector_etfs.pl < ticker_to_classifiations.csv |uniq > ticker_to_sec_etf.csv
    for etf in HHH IYR IYT OIH RKH RTH SMH UTH XLE XLF XLI XLK XLP XLV XLY; do grep $etf ticker_to_sec_etf.csv |wc -l; done
    # NB: get raw ticker/ETF pairing via:
    # cut -d',' -f1,8 ticker_to_sec_etf.csv 
    ## save tickers only in tickers_classified:
    cut -d',' -f1 ticker_to_sec_etf.csv|sed '1d' > tickers_classified
    wc -l tickers_classified 
    ## 1696 tickers_classified

    My equivalent of table 3 is given below:
    :DETAILS:
HHH  37   
IYR  76 
IYT  37 
OIH  49 
RKH  76 
RTH  71 
SMH  83 
UTH  78 
XLE  38 
XLF  158
XLI  244
XLK  273
XLP  75 
XLV  216
XLY  185
    :END:
** Data processing:
   Convert to a returns matrix sorted by date, ticker:
   # ./convert.py -i etf_data_ret.csv -o etf_old_ret_mtx
   ./convert.py -i etf_data_ret.csv -o etf_ret_mtx
   ./convert.py -i spx_data_ret.csv -o spx_ret_mtx
   ./convert.py -i univ1_data_ret.csv -o univ1_ret_mtx
   
   Correlation matrix: get rid of the tickers that have too many NAs
   proc_corr.r

* Signal generation
** Current format: list with 
   (1) list of dates and 
   (2) list of dates with a signal matrix attached
   Signal generation is performed via a command like
   sig.list.04.05 <- stock.etf.signals(ret.s,ret.e,tickers.classified,num.days=num.days,compact.output=TRUE)
   the compact.output=T is necessary to avoid a (giant) overhead of named
   attributes
#+BEGIN_SRC R
  ## compact output format:
  ## matrix with rows corresponding to stocks; each row is an unnamed numeric array A
  ## int2logical(A[1],5) gives logical w/ names corr to
  ## c("model.valid", "bto", "sto", "close.short", "close.long")
  ## A[2:8] are mr.params, names c("s","k","m","mbar","a","b","varz")
  ## A[9...] are betas (determined from stock names)
  For date i and ticker j, extract parameters from the matrix via something
  like
  sig <- decode.signals(signals[[i]][j,])
  params <- decode.params(signals[[i]][j,])
  betas <- decode.betas(signals[[i]][j,])
#+END_SRC scheme


* Backtesting
  Trading simulation: 
  select stocks to trade against ETFs/synthetic ETFs
  pre-generate signals
  go through dates in chronological order
  for every stock, examine signals
  Note that because the short-to-open/buy-to-close and
  buy-to-open/sell-to-close signals form bands above and below zero
  respectively, we are either short or long, never both.
** We also need to filter the beta-portfolio:
   - eliminate values that are less than B.THR percent of that maximum
     component in absolute value
   - eliminate negative values

** Trading process pseudocode:
 for every day: for every stock:
  if model.valid:
    if STO:
      if(!short): #flat or long (but shouldn't be long here)
	sell stock, buy factors #opening short (if flat before, as we should be)
	if(long): warning("STO tripped while long, weird jump?")
      else: do nothing #already short
    if CLOSE.SHORT:
      if(short): 
	buy stock, sell factors #closing short
	else: do nothing
    if BTO:
      if(!long): #flat or short (but shouldn't be short here)
        buy stock, sell factors #opening long
	if(short): warning("BTO tripped while short, weird jump?")
      else: do nothing #already long
    if CLOSE.LONG:
      if(long):
        sell stock, buy factors #closing long
      else: do nothing
** Determining transaction quantities
   We scale the investments in proportional to the current equity:
   Q[t] = Equity[t]*Lambda[t], where lambda is determined by the desired
   leverage (e.g. if expecting 100 long/100 short portfolio with 2+2
   leverage, lambda=2/100; cf page 22 of AL paper)
   For every stock and beta-portfolio component, we compute Q[t]/price,
   round, and get the number of shares.
** Trading setup:
   first, we need to create price tables from data
   for now, just use the convert python script with bid/asks instead of rets
   1-DATE; 9-TICKER; 28-RET; 29-BID; 30-ASK
*** shell commands to generate price tables
   cut -d ',' -f 1,9,29 univ1_data_full.csv > univ1_data_bid.csv
   cut -d ',' -f 1,9,30 univ1_data_full.csv > univ1_data_ask.csv
   ./convert-bid.py -i univ1_data_bid.csv -o univ1_bid_mtx
   ./convert-ask.py -i univ1_data_ask.csv -o univ1_ask_mtx
   cut -d ',' -f 1,9,29 etf1_data_full.csv > etf_data_bid.csv
   cut -d ',' -f 1,9,30 etf1_data_full.csv > etf_data_ask.csv
   ./convert-bid.py -i etf_data_bid.csv -o etf_bid_mtx
   ./convert-ask.py -i etf_data_ask.csv -o etf_ask_mtx
*** Work with mid-prices; here's the code to generate master mid-price tables
    test.ask <- get.mtx.gen("etf_ask_mtx",M=9*252,offset=offset.2009,file=TRUE)
    test.bid <- get.mtx.gen("etf_bid_mtx",M=9*252,offset=offset.2009,file=TRUE)
    stocks.mid.price <- (test.ask+test.bid)/2

    test.ask <- get.mtx.gen("etf_ask_mtx",M=9*252,offset=offset.2009,file=TRUE)
    test.bid <- get.mtx.gen("etf_bid_mtx",M=9*252,offset=offset.2009,file=TRUE)
    etf.mid.price <- (test.ask+test.bid)/2

    univ1.master.price <- cbind(etf.mid.price,stocks.mid.price)
   
** R issues with signal generation
   Data structures in R are extremely wasteful if you liberally use lists
   with mixed types and named objects.  This probably slows down the whole
   calculation significantly.  Temporary fix is to compact all the generated
   signals for a given date into a matrix; size is about 800K/400 stocks/25
   days
** Data offsets (assuming R data frames are reverse-chronologically sorted)
   This assumes data sets ending on 20090630
   which(as.logical(match(dates.vector,20090102)))
   ## 124
   which(as.logical(match(dates.vector,20080102)))
   ## 377
   which(as.logical(match(dates.vector,20070103)))
   ##  628
   which(as.logical(match(dates.vector,20060103)))
   ##  879
   which(as.logical(match(dates.vector,20050103)))
   ## 1131
   which(as.logical(match(dates.vector,20040102)))
   ## 1383
   which(as.logical(match(dates.vector,20030102)))
   ##  1635

   offset.2009 <- 124
   offset.2008 <- 377
   offset.2007 <- 628
   offset.2006 <- 879
   offset.2005 <- 1131
   offset.2004 <- 1383
   offset.2003 <- 1635

* Debugging backtesting
** First, need to ascertain that the returns datasets and the prices datasets are consistent
   -> did a spot check on XLF and JPM, the computed logreturns, returns, and
      reported returns are all consistent
** Isolated pair trading sequence: JPM and XLF -- examine the signals
   (Using 04-05 data)
   First signal:
56  56 pos: 0 ,inv.targ: 1000 ratio  0.80809  prices:  41.005 28.925  num shares:  103 -180 
BTO: 'acquiring' 103 -180  paying  -982.985 
beta.56 <- 1.237
What do we expect to happen if beta remains constant:
assuming alpha is negligible relative to mean-reverting contribution, we
   predict JPM prices from beta and XLF prices; the true price by the time
   the sell signal trips is expected to be higher due to positive increment
   in the mean-reverting Xt process.
88  88 pos: 103 ,inv.targ: 997.9351 ratio  0.7612402  prices:  36.475 27.48  num shares:  87 -152 
CLOSING LONG: paying  1189.475 
  Cash inflow is negative, so something went wrong
To examine the signals, take the debug output, save it to a file and extract
   the fields via somn like
 perl -lane 'print "$F[7],$F[9],$F[10]"' jpm.xlf.tmp > jpm.xlf.dbg1 ##OR:
 perl -lane 'print "$F[6],$F[8],$F[9]"' jpm.xlf.tmp > jpm.xlf.dbg1
